\title{
CPUアイドル時間管理
}

\author{
Copyright: (C) 2018 インテルコーポレーション
}

著者ラファエルJワイソッキ <rafael.j.wysocki@intel.com>

\section*{Concepts}

最近のプロセッサは一般に、プログラムの実行が中断され、そのプログラムに属す命令がメモリからフェッチされたり実行されたりしない状態に入ることができる。これらの状態はプロセッサのアイドル状態である。

アイドル状態では、プロセッサのハードウェアの一部は使用されないため、アイドル状態に入ることで、一般的にプロセッサの消費電力を削減することができ、結果的にエネルギーを節約する機会になる。

CPUアイドル時間管理は、この目的のためにプロセッサのアイドル状態を使用することに関係するエネルギー効率機能である。

\section*{Logical CPUs}

CPUアイドル時間管理は、CPUスケジューラ (システム内の計算作業の分配を担当するカーネルの部分)から見たCPUに対して行われる。その見解では、CPUは論理ユニットである。つまり、独立した物理的な実体である必要はなく、個々のシングルコアプロセッサとしてソフトウェアに表示されるインタフェースであってもよい。言い換えれば、CPUは、メモリから1つのシーケンス (プログラム)に属す命令をフェッチして実行しているように見える実体であるが、物理的にこのように動作する必要はない。一般に、ここでは3つの異なるケースが考えられる。

まず、プロセッサ全体が一度に1つの命令シーケンス (1つのプログラム)にしか従えない場合、それはCPUである。その場合、ハードウェアがアイドル状態に入るように要求されれば、それはプロセッサ全体に適用される。

第二に、プロセッサがマルチコアである場合、その中の各コアは一度に少なくとも1つのプログラムに従うことができる。コアは互いに完全に独立している必要はないが (例えば、キャッシュを共有する場合もある)、それでもほとんどの場合、互いに物理的に並行して動作するため、各コアが1つのプログラムだけを実行する場合、それらのプログラムは同時に互いにほとんど独立して実行される。その場合、コア全体がCPUであり、ハードウェアがアイドル状態に入るよう求められた場合、それは最初にアイドル状態を求めたコアに適用されるが、そのコアが属すより大きなユニット (「パッケージ」や「クラスタ」など)にも適用されることがある (実際には、そのコアを含むより大きなユニットの階層全体に適用されることもある)。すなわち、より大きなユニットに属すコアのうち、1つを除く全てのコアが「コアレベル」でアイドル状態になっており、残りのコアがプロセッサにアイドル状態になるよう要求した場合、それが引き金となってより大きなユニット全体がアイドル状態になり、そのユニット内の他のコアにも影響が及ぶ可能性がある。

最後に、マルチコアプロセッサの各コアは、同じ時間枠内で複数のプログラムに従うことができる場合がある (つまり、各コアは、メモリ内の複数の場所から命令をフェッチし、同じ時間枠内で実行できるが、必ずしも完全に互いに並行して実行する必要はない)。この場合、コアはソフトウェアに対して、ハードウェアスレッド (インテルハードウェアではハイパースレッド)と呼ばれる、それぞれが1つのシーケンスに従うことができる複数のシングルコア「プロセッサ」からなる「バンドル」として表示される。
ストラクションを使用する。そして、ハードウェアスレッドは、CPUアイドル時間管理の観点からはCPUであり、そのうちの1つによってプロセッサがアイドル状態に入るように要求された場合、それを要求したハードウェアスレッド (またはCPU)は停止されるが、同じコア内の他のハードウェアスレッドも全てプロセッサにアイドル状態に入るように要求していない限り、それ以上は何も起こらない。そのような状況では、コアが個別にアイドル状態になるか、コアを含むより大きなユニットが全体としてアイドル状態になる (その大きなユニット内の他のコアがすでにアイドル状態になっている場合)。

\section*{Idle CPUs}

論理CPU (以下では単に「CPU」と呼ぶ)は、特別な「アイドル」タスクを除き、実行するタスクがない場合、Linuxカーネルによってアイドルとみなされる。

タスクは、CPUスケジューラがジョブを表現するものである。各タスクは、実行する命令のシーケンス、つまりコードと、そのコードの実行中に操作するデータ、そしてタスクのコードがCPUによって実行されるたびにプロセッサにロードされる必要のあるコンテキスト情報から構成される。CPUスケジューラは、システムに存在するCPUに実行するタスクを割り当てることで、ジョブを分散させる。

タスクには様々な状態がある。特に、利用可能なCPUがある限り、そのコードがCPUによって実行されることを妨げる特別な条件がない場合 (例えば、イベントの発生を待っているなど)、実行可能な状態になる。タスクが実行可能になると、CPUスケジューラはそれを実行可能なCPUの1つに割り当てて実行させ、それ以上実行可能なタスクが割り当てられていなければ、CPUは与えられたタスクのコンテキストをロードしてそのコードを実行する (これまで最後に実行された命令の次の命令から、おそらく別の \(\mathrm{CPU}\) によって)。[一つのCPUに複数の実行可能なタスクが同時に割り当てられている場合、それらのタスクは優先度付けと時間共有の対象となり、時間の経過とともに何らかの進歩を遂げることができるようになる]。

特別な「アイドル」タスクは、与えられたCPUに他に実行可能なタスクが割り当てられていない場合に実行可能になり、そのCPUはアイドルとみなされる。言い換えると、LinuxのアイドルCPUは、アイドルループと呼ばれる「アイドル」タスクのコードを実行する。そのコードは、エネルギーを節約するために、サポートされていればプロセッサをアイドル状態の1つにする可能性があるが、プロセッサがアイドル状態をサポートしていなかったり、次のウェイクアップイベントまでにアイドル状態で過ごす時間が十分でなかったり、利用可能なアイドル状態のどれもが使用されないような厳しいレイテンシ制約がある場合、CPUは、実行する新しいタスクが割り当てられるまで、多かれ少なかれ無駄な命令をループで実行するだけである。

\section*{The Idle Loop}

アイドルループのコードは、その繰り返しごとに2つの主要なステップを踏む。まず、CPUIdleと呼ばれるCPUアイドル時間管理サブシステムに属すガバナーと呼ばれるコードモジュールを呼び出し、CPUがハードウェアにアイドル状態に入るよう求めるアイドル状態を選択する。次に、ドライバと呼ばれるCPUIdleサブシステムに属す別のコードモジュールを呼び出し、ガバナーによって選択されたアイドル状態に入るよう、プロセッサハードウェアに実際に要求する。

ガバナーの役割は、現在の状況に最も適したアイドル状態を見つけることである。この目的のために、論理CPUがハードウェアに入力させることができるアイドル状態は、プラットフォームやプロセッサアーキテクチャに依存しない抽象的な方法で表現され、1次元 (線形)配列で編成される。その配列には
を準備し、初期化時にカーネルが動作しているプラットフォームに合わせて CPUIdle ドライバから供給する。これにより、CPUIdleガバナーはプラットフォームとなるハードウェアから独立し、Linuxカーネルが実行可能なあらゆるプラットフォームで動作できる。

そのアレイに存在する各アイドル状態は、ガバナーが考慮すべき2つのパラメータ、ターゲットレジデンシと (最悪の)終了レイテンシによって特徴付けられる。目標残留時間は、ハードウェアが、より浅いアイドル状態の1つに入ることで節約できるエネルギーよりも多くのエネルギーを節約するために、その状態に入るために必要な時間 (かなりの時間可能性がある)を含めて、与えられた状態で過ごさなければならない最小時間である。[アイドル状態の「深さ」は、その状態でプロセッサが消費する電力にほぼ対応する]。出口レイテンシは、CPUがプロセッサハードウェアにアイドル状態へのマイグレーションを要求してから、その状態からのウェイクアップ後に最初の命令の実行を開始するまでにかかる最大時間のことである。一般に、終了レイテンシは、ハードウェアがその状態に入るときにウェイクアップが発生し、順序よく終了するために完全に入らなければならない場合に、与えられた状態に入るのに必要な時間もカバーしなければならないことに注意。

ガバナーの判断に影響を与える情報には2種類ある。まず、ガバナーは最も近いタイマイベントまでの時間を知っている。この時間は、カーネルがタイマをプログラムし、それがいつトリガされるかを正確に知っているため、正確に知られており、指定されたCPUが依存するハードウェアがアイドル状態で過ごすことができる最大時間である (アイドル状態へのマイグレーションと終了に必要な時間を含む)。しかし、CPUはいつでも (特に、最も近いタイマがトリガされる前に)タイマ以外のイベントによって起動される可能性があり、一般的にそれがいつ起こるかはわからない。ガバナーは、CPUがウェイクアップされた後、実際にどれだけの時間アイドル状態であったかを知ることができるだけであり (その時間を以後、アイドル継続時間と呼ぶ)、その情報を、最も近いタイマまでの時間と共に、将来のアイドル継続時間を推定するために何らかの形で使用できる。ガバナーがその情報をどのように使用するかは、そのガバナーがどのようなアルゴリズムを実装しているかに依存し、これがCPUIdleサブシステムに複数のガバナーが存在する主な理由である。

CPUIdleガバナーには、menu、TEO、ladder、haltpollの4種類がある。どれがデフォルトで使われるかは、カーネルの設定に依存し、特にアイドルループによってスケジューラーのティックを停止できるかどうかに依存する。利用可能なガバナーはavailable_governorsから読み込むことができ、実行時にガバナーを変更することもできる。カーネルが現在使用しているCPUIdleガバナーの名前は、sysfsの/sys/devices/system/cpu/cpuidle/の下にあるcurrent_governor_roまたはcurrent_governorファイルから読み取ることができる。

一方、どのCPUIdleドライバを使用するかは、通常、カーネルが動作しているプラットフォームに依存するが、複数の一致するドライバを持つプラットフォームもある。例えば、大多数のIntelプラットフォームで動作する2つのドライバ、intel_idleとacpi_idleがあり、1つはハードコードされたアイドル状態の情報を持ち、もう1つはシステムのACPIテーブルからその情報を読み取ることができる。しかし、そのような場合でも、システムの初期化時に選択したドライバを後で置き換えることはできないので、どちらを使用するかは早い段階で決定しなければならない (インテルプラットフォームでは、intel_idleが何らかの理由で無効になっているか、プロセッサを認識していない場合、acpi_idleドライバが使用される)。カーネルが現在使用しているCPUIdleドライバーの名前は、sysfsの/sys/devices/system/cpu/cpuidle/にあるcurrent_driverファイルから読み取ることができる。

\section*{Idle CPUs and The Scheduler Tick}

スケジューラティックは、CPUスケジューラのタイムシェアリング戦略を実行するために、定期的にトリガされるタイマである。もちろん、1つのCPUに複数の実行可能なタスクが同時に割り当てられている場合、与えられた時間枠の中で妥当な進捗をさせる唯一の方法は、利用可能なCPU時間を共有させることである。すなわち、大雑把に言えば、各タスクには、スケジューリングクラスや優先度付けなどの条件に従って、そのコードを実行するためのCPU時間のスライスが与えられ、そのタイムスライスを使い切ったら、CPUを別のタスク (のコード)の実行に切り替えるべきである。しかし、現在実行中のタスクは、自発的にCPUを手放したくない可能性がある。スケジューラティックは、それに関係なく切り替えを実現するために存在する。それがティックの唯一の役割ではないが、ティックを使う主な理由である。

スケジューラティックは、周期的かつ比較的頻繁にトリガされるため (カーネル構成によりますが、ティック期間の長さは \(1 \mathrm{~ms}\) と \(10 \mathrm{~ms})\) の間である)、CPUアイドル時間管理の観点から問題がある。したがって、アイドル状態のCPUでtickのトリガが許可されている場合、tick期間の長さを超えるターゲットレジデンシーでアイドル状態に入るようにハードウェアに要求することは意味をなさない。さらに、その場合、どのCPUのアイドル期間もtick期間長を超えることはなく、アイドル状態のCPUのtickウェイクアップによるアイドル状態の入出力のために使用されるエネルギーは無駄になる。

幸いなことに、アイドル状態のCPUでtickをトリガさせる必要はない。なぜなら、 (定義上)アイドル状態のCPUは、特別な "アイドル "タスクを除いて、実行するタスクがないからである。言い換えれば、CPUスケジューラの観点からは、アイドルCPUのCPU時間を使うのはアイドルループだけである。アイドルCPUの時間は複数の実行可能なタスク間で共有される必要がないため、与えられたCPUがアイドルであれば、ティックを使用する主な理由はなくなる。その結果、アイドルCPUでスケジューラのtickを完全に停止させることは原理的には可能である。

アイドルループでスケジューラのtickを停止することに意味があるかどうかは、ガバナーが何を期待しているかによって決まる。第一に、tick範囲内に別の (tickでない)タイマがトリガされる予定がある場合、tickを停止するのは明らかに時間の無駄である。第二に、もしガバナーがティック範囲内で非タイマのウェイクアップを予期している場合、ティックの停止は必要なく、有害でさえある可能性がある。すなわち、その場合、ガバナーは予想されるウェイクアップまでの時間内に目標残留時間を持つアイドル状態を選択するため、その状態は比較的浅くなる。その場合、ガバナーは深いアイドル状態を選択することはできない。それは、すぐにウェイクアップするというガバナー自身の期待に反するからだ。ウェイクアップが本当にすぐに起こるのであれば、ティックを止めるのは時間の無駄であり、この場合、タイマのハードウェアを再プログラムする必要がある。一方、ティックを停止してもウェイクアップがすぐに起こらない場合、ハードウェアはガバナーによって選択された浅いアイドル状態で不定の時間を過ごす可能性があり、これはエネルギーの無駄になる。従って、もしガバナーがtick範囲内で何らかのウェイクアップを期待しているのであれば、tickトリガを許可する方が良い。しかし、そうでない場合、ガバナーは比較的深いアイドル状態を選択するので、tickはCPUを早くウェイクアップしすぎないように停止すべきである。

いずれにせよ、ガバナーは何を期待しているかを知っており、スケジューラーのティックを停止するかどうかの決定はガバナーに属す。それでも、 (ループの前の繰り返しで)すでにティックが停止している場合は、そのままにしておいた方がよく、ガバナーはそれを考慮する必要がある。

カーネルは、アイドルループでスケジューラのtickを完全に停止しないように設定できる。これは、ビルド時のコンフィギュレーション (CONFIG_NO_HZ_IDLEコンフィギュレーションオプションをアンセットする)か、あるいは以下の方法で行うことができる。
コマンドラインでnohz=offを渡す。どちらの場合も、スケジューラーティックの停止が無効になっているため、それに関するガバナーの決定はアイドルループコードによって単に無視され、ティックが停止することはない。

アイドル状態のCPUでスケジューラのティックを停止できるように設定されたカーネルを実行するシステムは、ティックレスシステムと呼ばれ、一般に、ティックを停止できないカーネルを実行するシステムよりもエネルギー効率が高いとみなされる。与えられたシステムがティックレスであれば、デフォルトでメニューガバナを使用し、ティックレスでなければ、デフォルトのCPUIdleガバナがラダーになる。

\section*{The menu Governor}

メニューガバナーは、ティックレスシステムのデフォルトCPUIdleガバナーである。これは非常に複雑であるが、その設計の基本原理は単純である。すなわち、CPUのアイドル状態 (CPUがプロセッサハードウェアに入力を要求するアイドル状態)を選択するために呼び出されると、アイドル時間の予測を試み、アイドル状態の選択に予測値を使用する。

まず、スケジューラのティックが停止するという前提で、最も近いタイマイベントまでの時間を求める。この時間をスリープ長と呼び、次のCPUウェイクアップまでの時間の上限とする。これは、スリープ長補正係数を求めるために必要なスリープ長範囲を決定するために使用される。

メニューガバナーは、スリープ長補正係数の2つの配列を保持する。そのうちの1つは、指定されたCPU上で以前に実行されていたタスクがI/O処理の完了を待っているときに使用され、もう1つは、そうでないときに使用される。各配列には、異なるスリープ長範囲に対応する複数の補正係数の値が含まれており、配列で表される各範囲は、前のものよりも約10倍広くなるように編成されている。

与えられたスリープ長範囲の補正係数 (CPUのアイドル状態を選択する前に決定される)は、CPUがスリープ解除された後に更新され、スリープ長が観測されたアイドル持続時間に近いほど、補正係数は1に近くなる (0と1の間に収まらなければならない)。スリープ長に、それが該当する範囲の補正係数を乗じることで、予測アイドル時間の第一近似値が得られる。

次に、ガバナーは単純なパターン認識アルゴリズムを用いてアイドル時間の予測を精緻化する。すなわち、過去 8 回のアイドル持続時間の観測値を保存し、次回のアイドル持続時間を予測する際に、その平均と分散を計算する。分散が小さい (400平方ミリ秒より小さい)か、平均値に対して小さい (平均値が標準偏差の6倍より大きい)場合、平均値は「典型的な間隔」値とみなされる。そうでない場合は、保存されたアイドル時間値のうち最長のものが破棄され、残りのものに対して計算が繰り返される。この場合、「典型的な間隔」は「無限大」 (符号なし整数値の最大値)に等しいと仮定される。このようにして計算された「典型的な間隔」は、スリープ長に補正係数を掛けたものと比較され、両者の最小値が予測されるアイドル持続時間とされる。

次に、ガバナーは、「対話型」ワークロードを支援するために、追加のレイテンシ制限を計算する。選択されたアイドル状態の終了レイテンシが、予測されたアイドル時間と同程度である場合、その状態で費やされる総時間はおそらく非常に短く、その状態に入ることで節約されるエネルギー量は比較的小さいため、その状態に入って終了することに関連するオーバーヘッドを回避する方が良い可能性が高いという観測を利用する。したがって、より浅い状態を選択する方が良い選択肢である可能性が高い。余分なレイテンシ制限の最初の近似値は、予測されるアイドル時間そのものであり、さらに、与えられたCPU上で以前に実行され、現在I/O操作の完了を待っているタスクの数に応じた値で除算される。その除算結果は、電力管理QoSフレームワークから得られるレイテンシ制限と比較され、両者の最小値がアイドル状態の終了レイテンシの制限値として採用される。

さて、ガバナーはアイドル状態のリストを歩き、その中から1つを選択するレディ。この目的のために、各状態のターゲットレジデンシーを予測されたアイドル時間と比較し、その終了レイテンシを計算されたレイテンシ制限と比較する。予測されたアイドル持続時間に最も近いが、まだそれを下回るターゲットレジデンシーと、制限を超えない終了レイテンシを持つ状態を選択する。

最後のステップで、ガバナーは、まだスケジューラのティックを停止することを決定していない場合、アイドル状態の選択を改良する必要がある可能性がある。これは、ガバナーによって予測されたアイドル時間がtick期間より短く、tickが (アイドルループの前の反復で)まだ停止していない場合に起こる。その場合、以前の計算で使用されたスリープ長は、最も近いタイマイベントまでの実時間を反映していない可能性があり、もし本当にその時間より大きい場合、ガバナーは適切な目標残留時間を持つより浅い状態を選択する必要がある可能性がある。

\section*{The Timer Events Oriented (TEO) Governor}

タイマイベントオリエンテッド(TEO)ガバナーは、ティックレスシステム用の代替CPUIdleガバナーである。このガバナーもメニューガバナーと同じ基本戦略に従っている。しかし、この問題には異なるアプローチを適用する。

このガバナーのアイデアは、多くのシステムにおいてタイマイベントは他の割り込みよりも2桁以上頻繁に発生するため、アイドル状態からのCPUウェイクアップの最も重要な原因である可能性が高いという観察に基づいている。さらに、 (比較的最近の)過去に起こったことに関する情報を使用して、スリープ長 (sleep length)と呼ばれる、最も近いタイマイベントまでの (既知の)時間内に目標残留時間を持つ最も深いアイドル状態が、今後のCPUアイドル期間に適しているかどうかを推定し、適していない場合は、その代わりに、より浅いアイドル状態のどれを選択するかを決定できる。

もちろん、CPUの直近のアイドル時間間隔をいくつか考慮することでカバーできる使用例では、タイマ以外のウェイクアップソースの方が重要である。しかし、そのような場合であっても、スリープ時間よりも長いアイドル時間を考慮する必要はない。なぜなら、最も近いタイマが、より早くウェイクアップされない限り、最終的にCPUをウェイクアップするからである。

したがって、このガバナーは、CPUのアイドル時間がスリープ時間より大幅に短くなる可能性があるかどうかを推定し、それに応じてアイドル状態を選択する。

このガバナーによって実行される計算は、CPUIdleドライバによって提供されるCPUアイドル状態の目標残留パラメータ値と一致する境界を持つビンを昇順または降順で使用することに基づいている。
ダー。すなわち、最初のビンは、0から2番目のアイドル状態 (アイドル状態1)の目標滞留時間まで (ただし、アイドル状態1を含まない)、2番目のビンは、アイドル状態1の目標滞留時間からアイドル状態2の目標滞留時間まで (ただし、アイドル状態2を含まない)、3番目のビンは、アイドル状態2の目標滞留時間からアイドル状態3の目標滞留時間まで (ただし、アイドル状態3を含まない)、などのスパンである。最後のビンは、ドライバから供給された最も深いアイドル状態の目標残留数から無限大までのスパンである。

ヒット」と「インターセプト」と呼ばれる2つのメトリクスが各ビンに関連付けられている。これらは、前回起こったことに従って、与えられたCPUのアイドル状態を選択する前に、毎回更新される。

ヒット」メトリクスは、スリープ長および CPU ウェイクアップ後に測定されたアイドル時間が同じビンに入る (つまり、スリープ長に対して CPU が「時間通り」にウェイクアップする)状況の相対的な頻度を反映する。一方、「インターセプト」メトリクスは、測定されたアイドル時間がスリープ長よりも非常に短いため、スリープ長によってビンに入るアイドル状態よりも浅いアイドル状態に対応する状況の相対的な頻度を反映する (これらの状況を、以下では「インターセプト」と呼ぶ)。

上記のメトリクスに加えて、ガバナーは、各ビンの最近のインターセプト (つまり、指定されたCPUのためのそれの最後のNR_RECENT呼び出しの間に発生したインターセプト)をカウントする。

CPUのアイドル状態を選択するために、ガバナーは以下のステップを踏む (レイテンシ制約も考慮する必要がある)：

1.目標残留時間が現在のスリープ長を超えない最も深いCPUアイドル状態 (候補アイドル状態)を見つけ、以下のように3つの和を計算する：
- 候補状態および全ての深いアイドル状態の「ヒット」と「インターセプト」のメトリクスの合計 (スリープの長さが現在の長さと同じであった場合、インターセプトを回避するのに十分なほどCPUがアイドル状態であったケースを表す)。
- 候補のアイドル状態より浅い全てのアイドル状態の「インターセプト」メトリクスの合計 (スリープの長さが現在の長さと同じだった場合に、インターセプトされるのを避けるためにCPUが十分に長くアイドル状態でなかったケースを表す)。
- 候補の状態よりも浅い全てのアイドル状態の最近のインターセプト数の合計。

2.2番目の合計が1番目の合計より大きいか、3番目の合計がNR_RECENT / 2より大きい場合、CPUは早くウェイクアップする可能性が高いので、選択する別のアイドル状態を探す。
- 候補より浅いアイドル状態を降順に辿る。
- それぞれについて、「インターセプト」の指標の合計と、その候補となる候補の間の全てのアイドル状態 (前者を含み、後者を除く)における最近のインターセプトの数の合計を計算する。
- 考慮する必要があるこれらの合計の各々が (それに関連するチェックがCPUが早期に目覚める可能性があることを示したため)、ステップ1で計算された対応する合計の半分より大きい場合 (これは、該当するケースの半分以上において、当該状態の目標残留時間がアイドル持続時間を超えていなかったことを意味する)、候補のアイドル状態の代わりに所定のアイドル状態を選択する。

3.デフォルトでは、候補の状態を選択する。

ユーティリティを意識したメカニズム：

util-awareness拡張の背景にある考え方は、CPUには2つの異なるシナリオがあり、アイドル状態の選択には2つの異なるアプローチ (utilizedとnot utilized)があるはずだということだ。

この場合、「利用されている」とは、CPUの平均ランキュー利用率がある閾値を上回っていることを意味する。

CPUが利用されながらアイドルにマイグレーションする場合、より多くの作業を行うためにすぐにCPUが起動する可能性が高いため、レイテンシを最小化しパフォーマンスを最大化するために、より浅いアイドル状態を選択すべきである。CPUが使用されていないときは、電力節約を利用するために、利用可能な最も深いアイドル状態を選択する通常のメトリクスベースのアプローチを優先すべきである。

これを実現するために、ガバナーは利用率の閾値を使用する。この閾値は、CPUの容量値をビットシフトすることで、CPUの容量に対するパーセンテージとしてCPUごとに計算される。テストによると、6 (1.56%)のシフトが最良の結果を得ているようだ。

次のアイドル状態を選択する前に、ガバナーは現在のCPU利用率を事前に計算された利用率の閾値と比較する。もしそれ以下であれば、TEOメトリクスメカニズムがデフォルトとなる。それ以上の場合は、ポーリング状態でない限り、最も近い浅いアイドル状態が代わりに選択される。

\section*{Representation of Idle States}

CPUのアイドル時間管理のためには、プロセッサがサポートする全ての物理的なアイドル状態を、構造体cpuidle_stateオブジェクトの1次元配列として表現する必要がある。各構造体cpuidle_stateオブジェクトは、個々の (論理)CPUがプロセッサハードウェアに特定のプロパティのアイドル状態になるように要求できるようにする。プロセッサ内にユニットの階層がある場合、1つのstruct cpuidle_stateオブジェクトで、階層の異なるレベルのユニットがサポートするアイドル状態の組み合わせをカバーできる。その場合、その構造体のターゲットレジデンシーと終了レイテンシパラメータは、最も深いレベルのアイドル状態 (すなわち、他の全てのユニットを含むユニットのアイドル状態)の特性を反映しなければならない。

例えば、「モジュール」と呼ばれるより大きなユニットの中に2つのコアを持つプロセッサを想定し、一方のコアが「コア」レベルで特定のアイドル状態 (例えば「 \(\mathrm{X}\) 」)に入るようハードウェアに要求すると、もう一方のコアがすでにアイドル状態「 \(\mathrm{X}\) 」にある場合、モジュールがそれ自身の特定のアイドル状態 (例えば「MX」)に入ろうとするトリガになるとする。言い換えれば、「コア」レベルでアイドル状態「 \(\mathrm{X}\) 」を要求することは、「モジュール」レベルでアイドル状態「MX」まで深く入るライセンスをハードウェアに与えるが、これが起こる保証はない (アイドル状態「 \(\mathrm{X}\) 」を要求しているコアが、代わりにその状態で終わるだけ可能性がある)。その場合、アイドル状態 " \(\mathrm{X}\) " を表す struct cpuidle_state オブジェクトのターゲットレジデンシーには、モジュールのアイドル状態 " \(\mathrm{MX}\) " で過ごす最小時間 (アイドル状態に入るために必要な時間を含む)を反映させる必要がある。同様に、このオブジェクトの終了レイテンシパラメータは、モジュールのアイドル状態「MX」の終了時間 (通常はそのエントリ時間も)をカバーしなければならない。これは、ウェイクアップ信号からCPUが最初の新しい命令の実行を開始するまでの最大レイテンシ時間だからである (モジュール内の両方のコアが、モジュール全体として動作可能になるとすぐに命令を実行する準備が常に整っていると仮定して)。

しかし、プロセッサ内部のユニット階層の異なるレベル間で直接的な調整がないプロセッサも存在する。そのような場合、「コア」レベルでアイドル状態を要求しても、例えば「モジュール」レベルには何ら自動的に影響せず、CPUIdleドライバが階層の処理全体に責任を持つ。次に、アイドル状態オブジェクトの定義は完全にドライバに任されているが、それでもなお、プロセッサハードウェアが最終的に入るアイドル状態の物理的特性は、アイドル状態選択のためにガバナーによって使用されるパラメータに常に従わなければならない (例えば、そのアイドル状態の実際の終了レイテンシは、ガバナーによって選択されたアイドル状態オブジェクトの終了レイテンシパラメータを超えてはならない)。

上述したターゲットレジデンシと終了レイテンシのアイドル状態パラメータに加えて、アイドル状態を表すオブジェクトはそれぞれ、アイドル状態を説明する他のいくつかのパラメータと、その状態に入るようにハードウェアに要求するために実行する関数へのポインタを含む。また、各構造体cpuidle_stateオブジェクトには、対応する構造体cpuidle_state_usageがあり、指定されたアイドル状態の使用統計情報が含まれている。この情報は、sysfsを介してカーネルによって公開される。

システム内の各CPUに対して、sysfs内に/sys/devices/system/cpu/cpu<N>/cpuidle/ディレクトリがあり、初期化時に与えられたCPUに番号<N>が割り当てられる。このディレクトリには、与えられたCPUに対して定義されたアイドル状態オブジェクトの数から1を引いた数まで、state0、statelなどと呼ばれるサブディレクトリのセットが含まれる。これらのディレクトリはそれぞれ1つのアイドル状態オブジェクトに対応し、その名前に含まれる数字が大きいほど、それによって表される (実効的な)アイドル状態が深くなる。各ディレクトリには、対応するアイドル状態オブジェクトのプロパティを表すファイル (アトリビュート)が、以下のように多数含まれている：

上記

このアイドル状態が要求された回数の合計であるが、観測されたアイドル持続時間は、目標残留時間と一致するには短すぎたことは確かである。

以下

このアイドル状態が要求された回数の合計であるが、確かに、より深いアイドル状態の方が、観測されたアイドル持続時間と一致しただろう。

降下

アイドル状態の説明。

無効にする

このアイドル状態が無効かどうか。

デフォルトステータス

この状態のデフォルトの状態、"enabled "または "disabled"。

\section*{latency}

アイドル状態の終了レイテンシ (マイクロ秒)。

名称

アイドル状態の名前。

\section*{power}

アイドル状態のハードウェアが消費する電力 (ミリワット) (指定された場合、それ以外は 0)。
居住

アイドル状態の目標滞在時間 (マイクロ秒)。

時間

指定されたCPUがアイドル状態で過ごした合計時間 (カーネルによって測定される) (マイクロ秒単位)。

\section*{usage}

ハードウェアが、指定されたCPUからこのアイドル状態に入るように要求された回数の合計。

却下

指定されたCPUのアイドル状態へのマイグレーション要求が拒否された回数の合計。

descファイルとnameファイルはどちらも文字列を含む。両者の違いは、nameはより簡潔であることが期待されるのに対し、descriptionはより長く、空白や特殊文字を含む可能性があることである。上記の他のファイルには整数が含まれる。

disable属性は書き込み可能な唯一の属性である。これは、ガバナーがこの特定のCPUに対してアイドル状態を選択することがなく、CPUIdleドライバがそのCPUに対してアイドル状態を入力するようハードウェアに要求することがないことを意味する。しかし、1つのCPUに対してアイドル状態を無効にしても、他のCPUからアイドル状態を要求されることを防ぐことはできないため、どのCPUからもアイドル状態を要求されないようにするには、全てのCPUに対してアイドル状態を無効にする必要がある。[ラダーガバナーが実装されているため、アイドル状態を無効にすると、無効化されたアイドル状態より深いアイドル状態も選択できなくなることに注意]。

disable属性が0を含む場合、指定されたアイドル状態はこの特定のCPUに対して有効であるが、システム内の他のCPUの一部または全てに対して同時に無効化される可能性がある。1を書き込むと、この特定のCPUに対してアイドル状態が無効になり、0を書き込むと、ドライバでその状態がグローバルに無効にされていない限り (その場合は全く使用できない)、ガバナーが指定されたCPUに対してアイドル状態を考慮し、ドライバがアイドル状態を要求できるようになる。

電力属性は、特にプロセッサ内のユニット階層の異なるレベルにあるアイドル状態の組み合わせを表すアイドル状態オブジェクトの場合、あまりうまく定義されておらず、一般的に複雑なハードウェアのアイドル状態の電力数値を取得するのは困難であるため、電力には0 (使用不可)が含まれることが多く、0以外の数値が含まれる場合、その数値はあまり正確でない可能性があり、意味のあるものについては信頼すべきではない。

なぜなら、この数値はカーネルによって測定されたものであり、ハードウェアがアイドル状態に入ることを拒否し、代わりにもっと浅いアイドル状態に入った (あるいはまったくアイドル状態に入らなかった)ケースをカバーしていない可能性があるからである。カーネルが計測できるのは、ハードウェアにアイドル状態へのマイグレーションを要求してからCPUがウェイクアップするまでの時間だけであり、その間にハードウェアレベルで実際に何が起こったかを語ることはできない。さらに、問題のアイドル状態のオブジェクトが、プロセッサ内のユニットの階層の異なるレベルでのアイドル状態の組み合わせを表している場合、カーネルは、特定のケースでハードウェアがどの程度深い階層に行ったかを言うことはできない。これらの理由から、ハードウェアがサポートする様々なアイドル状態でどれだけの時間が費やされたかを知る唯一の信頼できる方法は、ハードウェア内のアイドル状態残留カウンタが利用可能であれば、それを使用することである。

一般に、アイドル状態に入ろうとしたときに割り込みを受信すると、アイドル状態入力要求が拒否される。この場合、CPUIdleドライバはそのことを示すエラーコードを返す。この場合、CPUIdleドライバはエラーコードを返すことがある。usageファイルとrejectedファイルは、それぞれ、指定されたアイドル状態の入力に成功した回数と拒否された回数を報告する。

\section*{Power Management Quality of Service for CPUs}

Linuxカーネルの電力管理QoS (Quality of Service)フレームワークは、カーネルコードとユーザ空間プロセスが、カーネルの様々なエネルギー効率機能に制約を設定し、パフォーマンスが必要なレベル以下に低下するのを防ぐことを可能にする。

CPUのアイドル時間管理は、グローバルなCPUレイテンシ制限と個々のCPUのレジュームレイテンシ制約の2つの方法でPM QoSの影響を受けることができる。カーネルコード(例えばデバイスドライバ)はPM QoSフレームワークが提供する特別な内部インタフェースの助けを借りてこの2つを設定できる。ユーザ空間は / dev/ の下にある cpu_dma_latency 特殊デバイスファイルを開き、バイナリ値(符号付き32ビット整数として解釈)を書き込むことで、前者を変更できる。一方、CPUのレジュームレイテンシ制約をユーザ空間から変更するには、sysfsの/sys/devices/system/cpu/cpu<N>/の下にあるpower/pm_qos_resume_latency_usファイルに文字列 (符号付き32ビット整数を表す)を書き込みます。どちらの場合も負の値は拒否され、またどちらの場合も、書き込まれた整数はマイクロ秒単位で要求された PM QoS 制約として解釈される。

しかし、要求された値は自動的に新しい制約として適用されるわけではない。なぜなら、その値は以前に他の誰かによって要求された別の制約よりも制限が緩い(この特定のケースでは大きい)可能性があるからである。このため、PM QoS フレームワークはグローバルな CPU レイテンシ制限と個々の CPU に対してこれまでになされたリクエストのリストを保持し、それらを集約し、有効な(この特定のケースでは最小の)値を新しい制約として適用する。

実際、特殊デバイスファイル cpu_dma_latency をオープンすると、新しい PM QoS 要求が生成され、CPU レイテンシ制限要求のグローバルな優先度リストに追加される。そのファイルディスクリプタが書き込みに使用された場合、そのファイルディスクリプタに書き込まれた数値が、そのファイルディスクリプタが表す PM QoS 要求に関連付けられ、新しい要求限界値となる。次に、プライオリティリストメカニズムはリクエストのリスト全体の新しい実効値を決定するために使用され、その実効値が新しいCPUレイテンシリミットとして設定される。このように、新しいリミット値をリクエストすることは、有効な「リスト」値がその影響を受ける場合にのみ、実際のリミット値を変更する。

cpu_dma_latency 特殊デバイスファイルをオープンして得られたファイル記述子を保持するプロセスは、そのファイル記述子に関連する PM QoS 要求を制御するが、この特定の PM QoS 要求のみを制御する。

cpu_dma_latency 特殊デバイスファイル、より正確にはそれを開いている間に得られたファイル記述子を閉じると、そのファイル記述子に関連付けられた PM QoS リクエストは CPU レイテンシ制限リクエストのグローバルな優先度リストから削除され、破棄される。その場合、優先度リストのメカニズムが再び使用され、リスト全体の新しい有効値が決定され、その値が新しい制限値となる。

順番に、各CPUに対して、sysfsの/sys/devices/system/cpu/cpu<N>/の下にある power/pm_qos_resume_latency_us ファイルに関連付けられた1つのレジュームレイテンシPM QoS要求があり、これに書き込むと、どのユーザ空間のプロセスがそれを行うかに関係なく、この1つのPM QoS要求が更新される。言い換えると、この PM QoS リクエストはユーザ空間全体で共有されているので、混乱を避けるために関連するファイルへのアクセスは調停される必要がある。[議論の余地なく、このメカニズムの実際の唯一の正当な使い方は、プロセスを問題のCPUに固定し、そのためにレジュームレイテンシ制約を制御するために sys fs インタフェースを使わせることである]。しかしながら、それはまだリクエストに過ぎない。それは、リクエストのリストがこうして更新されるたびに、 問題のCPUのレジュームレイテンシ制約として設定される実効値を決定するために使用される優先度リストのエントリである(そのリストにはカーネルコードから来る他のリクエストがある可能性がある)。

CPUアイドル時間ガバナーは、与えられたCPUのためのグローバル (実効)CPUレイテンシ制限と実効レジュームレイテンシ制約の最小値を、そのCPUのために選択することが許されるアイドル状態の終了レイテンシの上限とみなすことが期待される。その制限を超えるレイテンシを持つアイドル状態を決して選択すべきではない。

\section*{Idle States Control Via Kernel Command Line}

sysfsインタフェースで個々のCPUのアイドル状態を無効にできるのに加えて、CPUアイドル時間管理に影響を与えるカーネルコマンドラインパラメータがある。

 \(f=1\) カーネルコマンドラインオプションのcpuidle . を使用すると、CPUアイドル時間管理を完全に無効にできる。これは、アイドル状態のCPUでアイドルループが実行されるのを防ぐことはできないが、CPUアイドルタイムガバナーやドライバが呼び出されるのを防ぎます。これがカーネルコマンドラインに追加されると、アイドルループは、この目的のためにデフォルトのメカニズムを提供することが期待されるCPUアーキテクチャのサポートコードを介して、アイドルCPU上でアイドル状態に入るようにハードウェアに要求する。しかし、そのデフォルトメカニズムは、当該アーキテクチャ (すなわちCPU命令セット)を実装する全てのプロセッサの最小公倍数であるのが普通であるため、かなり粗雑で、エネルギー効率もあまりよくない。このため、本番での使用は推奨されない。

cpuidle.governor=カーネルコマンドラインスイッチにより、使用するCPUIdleガバナーを指定できる。利用可能なガバナー名と一致する文字列 (例：cpuidle.governor=menu)を付加する必要があり、デフォルトのガバナーの代わりにそのガバナーが使用される。例えば、ラダーガバナーをデフォルトで使用しているシステムで、メニューガバナーを強制的に使用できる。

以下に説明するCPUアイドル時間管理を制御するその他のカーネルコマンドラインパラメータは、 \(x 86\) アーキテクチャにのみ関連し、intel_idleへの言及はインテルプロセッサのみに影響する。

 \(x 86\) アーキテクチャサポートコードは、CPUアイドル時間管理に関連する3つのカーネルコマンドラインオプション (idle=poll、idle=halt、idle=nomwait)を認識する。このうち最初の2つは、acpi_idleドライバとintel_idleドライバを完全に無効にする。これにより、CPUIdleサブシステム全体が事実上無効になり、アイドルループがアーキテクチャサポートコードを呼び出してアイドルCPUを処理するようになる。それがどのように行われるかは、カーネルコマンドラインに2つのパラメータのどちらを追加するかによって決まる。idle=haltの場合、アーキテクチャサポートコードはこの目的のためにCPUのHLT命令 (これは原則としてプログラムの実行を一時停止し、ハードウェアに利用可能な最も浅いアイドル状態に入ろうとさせる)を使用し、idle=pollが使用された場合、アイドル状態のCPUはタイトループで多かれ少なかれ「軽量な」命令シーケンスを実行する。注
idle=pollを使用することは、アイドル状態のCPUがほとんどエネルギーを節約しないようにすることだけが唯一の効果ではない可能性があるので、多くの場合、 多少思い切ったことである。例えば、Intelハードウェアでは、CPUがPステート(CPU Performance Scalingを参照)を使用することを効果的に防ぎます。したがって、パフォーマンス上の理由でこれを使うのは、まったく良い考えではない可能性がある]。

idle=nomwaitオプションは、CPUがアイドル状態に入るためのMWAIT命令の使用を防ぐ。このオプションが使用されると、 acpi_idleドライバは、MWAITの代わりにHLT命令を使用する。Intelプロセッサを実行しているシステムでは、このオプションはintel_idleドライバを無効にし、代わりにacpi_idleドライバを強制的に使用する。いずれの場合も、acpi_idleドライバが必要とする全ての情報がシステムのACPIテーブルにある場合にのみ、acpi_idleドライバが機能することに注意。

CPUアイドル時間管理に影響するアーキテクチャレベルのカーネルコマンドラインオプションに加えて、カーネルコマンドラインを介して渡すことができる、個々のCPUIdleドライバに影響するパラメータがある。具体的には、intel_idle.max_cstate=<n>とprocessor.max_cstate \(=<n>\) パラメータで、<n>はsys fsの指定された状態のディレクトリ名にも使用されているアイドル状態のインデックスである (アイドル状態の表現参照)。この場合、これらのアイドル状態を要求したり、 ガバナーに公開したりすることはない。[\(<n>\) が0の場合、2つのドライバの動作は異なる。一方、processor.max_cstate \(=0\) はprocessor.max_cstate \(=1\) と等価である。また、acpi_idleドライバはproces sorカーネルモジュールの一部であり、個別にロードすることができ、ロード時にモジュールパラメータとしてmax_cstate=<n>を渡すことができる]。
